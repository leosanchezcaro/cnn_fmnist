{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac62e205-45c7-4085-b6db-d4f67d55320d",
   "metadata": {},
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORKS APPLIED TO THE FASHION-MNIST DATASET || Mini-ResNet Model\n",
    "\n",
    "This project comprises the complete workflow for the supervised classification of apparel images. The dataset used is Fashion-MNIST (https://github.com/zalandoresearch/fashion-mnist), which consists of 28×28 grayscale images of 10 different clothing categories.\\\n",
    "As an alternative to the standard CNN model also presented in this repo, this notebook showcases a ResNet-like model which introduces residual connections to improve learning stability and convergence in a deeper network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0375bd-67ee-4a50-a499-0028f6798c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries and dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad3904-d33b-4b7a-af1f-8f743b87d5df",
   "metadata": {},
   "source": [
    "## DATA LOADING AND PRE-PROCESSING\n",
    "\n",
    "The Fashion-MNIST dataset is loaded using TensorFlow's built-in API.\\\n",
    "As it is best practice for this type of model, pixel values are normalized to [0,1]. Then, the training set is split into training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0972a488-9fa0-4350-afa8-436d97e027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset directly from TF.Keras API\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Visualizar 10 imágenes aleatorias de los conjuntos de entrenamiento y testeo\n",
    "\n",
    "# Randomly sample 10 images from each set\n",
    "train_indices = np.random.choice(len(train_images), size=10, replace=False)\n",
    "test_indices = np.random.choice(len(test_images), size=10, replace=False)\n",
    "\n",
    "# Create figure with 2 rows and 10 columns\n",
    "fig, axs = plt.subplots(2, 10, figsize=(20, 5))\n",
    "\n",
    "for i, idx in enumerate(train_indices):\n",
    "    axs[0, i].imshow(train_images[idx], cmap='magma_r')\n",
    "    axs[0, i].axis('off')\n",
    "    axs[0, i].set_title(f\"Train {i+1}\")\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    axs[1, i].imshow(test_images[idx], cmap='Blues')\n",
    "    axs[1, i].axis('off')\n",
    "    axs[1, i].set_title(f\"Test {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae40b2-6e72-444d-a1de-bb6a72314ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images, so the value of each pixel is comprised within [0,1] instead of [0,255]\n",
    "\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Check normalization\n",
    "print(f\"Dimensions: {train_images[0].shape}\")\n",
    "print(f\"normalized value range: {train_images[0].min()} - {train_images[0].max()}\")\n",
    "\n",
    "plt.imshow(train_images[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1e1a9-af19-46de-90a7-769a81939711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "\n",
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "\n",
    "# Divide 80% for training and 20% for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12bd1a-2bfb-4971-a062-3a8ef3bd3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance between train and validation datasets\n",
    "\n",
    "unique_classes = sorted(np.unique(np.concatenate((y_train, y_val))))\n",
    "\n",
    "train_counts = [np.sum(y_train == i) for i in unique_classes]\n",
    "val_counts = [np.sum(y_val == i) for i in unique_classes]\n",
    "\n",
    "x = np.arange(len(unique_classes))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x - width/2, train_counts, width, label='Training')\n",
    "ax.bar(x + width/2, val_counts, width, label='Validation')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(unique_classes)\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Sample size\")\n",
    "ax.set_title(\"Class distribution in training and validation datasets\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e9831-9edf-4532-8ae5-662ed3fc7c33",
   "metadata": {},
   "source": [
    "## DEEP RESIDUAL CNN ARCHITECTURE (ResNet-inspired)\n",
    "\n",
    "This model builds upon the previous CNN by introducing residual blocks, a core component of the ResNet family.\\\n",
    "This allows the model to have a deeper architecture than the standard CNN, and it's intended to better capture complex hierarchical features in the FashionMNIST dataset, while avoiding the vanishing gradient problem.\\\n",
    "Therefore, it is encouraged to continue adding layers and explore the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341f149-6324-46ad-b18a-9ab6f1914161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet\n",
    "\n",
    "def residual_block(x, filters, l2_value=0.001):\n",
    "    shortcut = x\n",
    "\n",
    "    if x.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, kernel_size=(1, 1), padding='same', \n",
    "                          kernel_regularizer=regularizers.l2(l2_value))(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same', \n",
    "               kernel_regularizer=regularizers.l2(l2_value))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), padding='same', \n",
    "               kernel_regularizer=regularizers.l2(l2_value))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Define model\n",
    "input_layer = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# Initial convolutional layer\n",
    "x = Conv2D(32, kernel_size=(3, 3), padding='same')(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Residual block 1\n",
    "x = residual_block(x, 32)\n",
    "x = residual_block(x, 32)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Residual block 2\n",
    "x = residual_block(x, 64)\n",
    "x = residual_block(x, 64)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "# Residual block 3\n",
    "x = residual_block(x, 128)\n",
    "x = residual_block(x, 128)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Residual block 4\n",
    "x = residual_block(x, 256)\n",
    "x = residual_block(x, 256)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "# Residual block 5\n",
    "x = residual_block(x, 512)\n",
    "x = residual_block(x, 512)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Dense layers\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "x = Dense(1024)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "output_layer = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ccb4f1-d836-4b44-8a4e-5c6eaf4920a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Compile model\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=learning_rate),\n",
    "    loss=SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e8cacf-add5-47eb-9a6c-2391ef5b18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler: reduce learning rate if validation loss plateaus\n",
    "lr_schedule = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Optional: save best model\n",
    "\n",
    "model_save_path = 'path/model.keras'  # Insert chosen path\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "   'models/ResNet.keras',  # Adjust path as needed\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [lr_schedule]  # Add checkpoint if desired: callbacks = [lr_schedule, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1424c-fde7-4dcc-8492-331cd7e4f412",
   "metadata": {},
   "source": [
    "## TRAINING, VALIDATION AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0de1e3-b083-4dbc-a034-8ceac5b08d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[callbacks],\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f6e97-ef73-413a-baba-3d082faed58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss during training and validation\n",
    "\n",
    "# Extract data from history object\n",
    "history_dict = history.history\n",
    "\n",
    "# Loss chart\n",
    "\n",
    "x_step = 5 # To adjust ticks on x\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_dict['loss'], label='training')\n",
    "plt.plot(history_dict['val_loss'], label='validation')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(x_step))\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy chart\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_dict['accuracy'], label='training')\n",
    "plt.plot(history_dict['val_accuracy'], label='validation')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.gca().xaxis.set_major_locator(plt.MultipleLocator(x_step))\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e323f-f75a-4220-b4e0-91b9d6676409",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION\n",
    "\n",
    "The trained model is evaluated on the validation set to assess its performance using confusion matrix and sample predictions.\n",
    "\n",
    "Note: the pre-trained model is not included in the repo due to GitHub's file size limitations. You can download it using this link:\\\n",
    "[Download MiniResNet.keras](https://drive.google.com/file/d/1yqQC_CTU_vCvAYyxe1EkbuVJHBP8IMxQ/view?usp=drive_link)\n",
    "\n",
    "Once downloaded, place the file inside the `models/` directory in your local project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d9474-c309-4620-9c89-60c2c497ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Load pre-trained model\n",
    "\n",
    "# model = load_model('models/MiniResNet.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbcd7a-441e-449e-80d6-69add20c471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict over validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_pred = y_val_pred.argmax(axis=1)  # Convertir probabilidades en clases enteras\n",
    "\n",
    "# Generate report\n",
    "report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "\n",
    "# Convert report to DataFrame\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Show in table format\n",
    "print(report_df)\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Show matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de confusión - Conjunto de validación')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713b4a80-4733-4693-80ce-b4a137d38ca7",
   "metadata": {},
   "source": [
    "### GENERATE SUBMISSION FILE (Optional)\n",
    "\n",
    "If you want to participate in a competition or hackathon (like [this one](https://www.analyticsvidhya.com/datahack/contest/practice-problem-identify-the-apparels/)), use the following code to generate a CSV with predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df2111-8287-4140-84a9-614dbc8e828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict over test dataset (if you intend to submit your results to the hackaton)\n",
    "\n",
    "# Ensure all 4 dimensions exist (if not, add batch size channel)\n",
    "if X_test.ndim == 3:\n",
    "    X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_labels = y_test_pred.argmax(axis=1)\n",
    "\n",
    "# Generate DataFrame with label column\n",
    "sub_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'label': y_test_labels\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "sub_df.to_csv('submission_cnn.csv', index=False)  # Change path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343d48f-e13b-4064-99c1-678611b716c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick assessment of model predictions\n",
    "\n",
    "labels = {0 : \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n",
    "          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n",
    "\n",
    "# Select 10 random indexes from the test dataset\n",
    "random_indices = random.sample(range(len(X_test)), 20)\n",
    "\n",
    "# Visualize samples with their corresponding predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(X_test[idx].reshape(28, 28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    pred_label = labels[y_test_labels[idx]]\n",
    "    plt.title(f\"Predicted: {pred_label}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
